## 优缺点/总结
- CNN没法学习得到长距离的词语之间的互动
- 用self-attn解决相关性
## 模型
### 结构
- news encoder
  - layer1：title word embedding，同[20],[21]，得到$[e_1,...,e_M]$，$M$为标题中的单词数
  - layer2：self-attn，$[e_1,...,e_M]$映射为query、key和value，最终$e_i$得到$k$个表示，k为attn的层数，然后将k层表示连接在一起，最终得到$[h_1,...,h_M]$
  - layer3：word-level attn：同[20,21]，计算得最终新闻表达$r$
- user encoder
  - layer1：news-level self-attention，同上，把最终得到的k个表示拼接起来得到$h_i$
  - layer2：news-level attn，找出比较重要的新闻，最终得到用户画像$u$

### tricks
### prediction/rating
### Objective Function/Loss Function
### 参数
### optimizer
## 问题