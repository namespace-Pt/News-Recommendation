## 模型
- bandit algorithm：A面临一个全新的环境，选择不同的动作会给A带来不同的反馈，A会在exploit当前动作和explore新的动作之间决策，bandit algorithm要找到让A的总regret最小的决策
- context-free bandit：不管动作的属性和A的属性，就是随机探索，有$\epsilon-greedy$，UCB1等算法
  - $\epsilon-greedy$，寻找当前估计的回报最高的arm；没看原理
  - UCB，寻找当前估计的回报-实际回报处于置信区间的可能性最大的arm
- contextual-bandit：exploration and exploitation have to be deployed at an individual level since the views of different users on the same content can vary signiﬁcantly. 
  - EXP4：regret达到$\Theta(\sqrt{T})$，但是计算呈现指数
  - epoch-greedy：在**oracle optimizer**加持下可以快速计算，但是regret达到$\Theta(T&{\frac{2}{3}})$
  - based on Bayes rule：需要线下工程，要和approximation technique配合

- LinUCB:
  - 计算复杂度关于number of arm线性，最多是特征数的立方
  - 对于动态的arm set，在$\mathcal{A}_t$不过于大的情况下一直保持高效，新闻池可以根据时间进行维护，使其保持大小
  - 见笔记

## Feature
- user
  - 定义support：拥有这个特征的用户的比例，只选取support>0.1的属性作为特征
  1. 性别，年龄段
  2. 地理位置
  3. 看过的新闻的类别
- article：
  1. 从文章URL推断的新闻类别
  2. 人工标注的主题

- 将所有user向量补齐维度，向量的每一个元素+1
- 将所有article向量补齐维度，向量的每一个元素+1
- user：1193维，article：83维
## 评估
- 收集下来的数据集只记录了用户的那一次选择，而算法算出来的选择很可能不同于记录下来的选择
- 将算法算出的选择与记录选择不同的时候都忽视，但具体没看


## 优缺点/总结
- 新闻内容一直在变，协同过滤没法做
- 新闻推荐速度必须快（学习和计算）
- 传统的bandit算法不考虑语义信息
## 解决的问题
## 模型
- 将新闻推荐建模成一个n-arm bandit问题，对每一个时刻的用户和其可以选择的n个新闻，都有对应的n个context向量，将选择其中某一条的期望建模成对应context的线性组合
### 结构
### tricks
### prediction/rating
### Objective Function/Loss Function
### 参数
### optimizer
## 问题