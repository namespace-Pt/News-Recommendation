{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bit4c03300bedca44f8b0013abe02048abc",
   "display_name": "Python 3.7.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### BenchMark in Interaction baselines, FIM excluded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import evaluate,train,prepare\n",
    "from models.baseline_CNN_CNN import GCAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'scale':'demo',\n",
    "    'name':'baseline-cnn-cnn',\n",
    "    'batch_size':10,\n",
    "    'title_size':20,\n",
    "    'his_size':50,\n",
    "    'npratio':4,\n",
    "    'dropout_p':0.2,\n",
    "    'query_dim':200,\n",
    "    'embedding_dim':300,\n",
    "    'filter_num':400,\n",
    "    'value_dim':16,\n",
    "    'head_num':16,\n",
    "    'epochs':5,\n",
    "    'metrics':'group_auc,ndcg@5,ndcg@10,mean_mrr',\n",
    "    'device':'cuda:0',\n",
    "    'attrs': ['title'],\n",
    "    'k':None,\n",
    "    'save_step':None,\n",
    "    'save_each_epoch':False,\n",
    "    'train_embedding':False,\n",
    "}\n",
    "\n",
    "device = torch.device(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, loader_train, loader_test, loader_validate = prepare(hparams, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcaModel = GCAModel(hparams, vocab=vocab).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCAModel(nn.Module):\n",
    "    def __init__(self,hparams,vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cdd_size = (hparams['npratio'] + 1) if hparams['npratio'] > 0 else 1\n",
    "        self.metrics = hparams['metrics']\n",
    "        self.device = torch.device(hparams['device'])\n",
    "        self.embedding = vocab.vectors.to(self.device)\n",
    "\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.signal_length = hparams['title_size']\n",
    "        self.his_size = hparams['his_size']\n",
    "\n",
    "        self.dropout_p = hparams['dropout_p']\n",
    "\n",
    "        self.filter_num = hparams['filter_num']\n",
    "        self.embedding_dim = hparams['embedding_dim']\n",
    "       \n",
    "        # elements in the slice along dim will sum up to 1 \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.DropOut = nn.Dropout(p=hparams['dropout_p'])\n",
    "        \n",
    "        self.CNN = nn.Conv1d(in_channels=self.embedding_dim,out_channels=self.filter_num,kernel_size=3,padding=1)\n",
    "        self.SeqCNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=(3,3)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=(3,3))\n",
    "        )\n",
    "        \n",
    "        # 64 is derived from SeqCNN\n",
    "        self.learningToRank = nn.Linear(144, 1)\n",
    "        # self.learningToRank = nn.Linear(self.repr_dim * self.his_size, 1)\n",
    "\n",
    "    def _scaled_dp_attention(self,query,key,value):\n",
    "        \"\"\" calculate scaled attended output of values\n",
    "        \n",
    "        Args:\n",
    "            query: tensor of [*, query_num, key_dim]\n",
    "            key: tensor of [batch_size, *, key_num, key_dim]\n",
    "            value: tensor of [batch_size, *, key_num, value_dim]\n",
    "        \n",
    "        Returns:\n",
    "            attn_output: tensor of [batch_size, *, query_num, value_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure dimension matches\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        key = key.transpose(-2,-1)\n",
    "\n",
    "        attn_weights = torch.matmul(query,key)/torch.sqrt(torch.tensor([self.embedding_dim],dtype=torch.float,device=self.device))\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights,value)\n",
    "        return attn_output\n",
    "\n",
    "    def _news_encoder(self,news_batch):\n",
    "        \"\"\" encode batch of news with 1d-CNN\n",
    "        \n",
    "        Args:\n",
    "            news_batch: tensor of [batch_size, *]\n",
    "        \n",
    "        Returns:\n",
    "            news_emebdding: tensor of [batch_size, *, filter_num] \n",
    "        \"\"\"\n",
    "\n",
    "        news_embedding = self.embedding[news_batch].transpose(-2,-1).view(-1,self.embedding_dim,news_batch.shape[-1])\n",
    "        news_embedding = self.CNN(news_embedding).transpose(-2,-1).view(news_batch.shape + (self.filter_num,))\n",
    "        news_embedding = self.ReLU(news_embedding)\n",
    "\n",
    "        if self.dropout_p > 0:\n",
    "            news_embedding = self.DropOut(news_embedding)\n",
    "\n",
    "        return news_embedding\n",
    "        \n",
    "    def _fusion(self, cdd_news_embedding, his_news_embedding):\n",
    "        \"\"\" concatenate candidate news title and history news title\n",
    "        \n",
    "        Args:\n",
    "            cdd_news_embedding: tensor of [batch_size, cdd_size, signal_length, filter_num] \n",
    "            his_news_embedding: tensor of [batch_size, his_size, signal_length, filter_num] \n",
    "\n",
    "        Returns:\n",
    "            fusion_news: tensor of [batch_size, cdd_size, his_size, signal_length, signal_length]\n",
    "        \"\"\"\n",
    "\n",
    "        fusion_matrices = torch.matmul(cdd_news_embedding.unsqueeze(dim=2), his_news_embedding.unsqueeze(dim=1).transpose(-2,-1)).view(self.batch_size * self.cdd_size * self.his_size, 1, self.signal_length, self.signal_length)\n",
    "        fusion_vectors = self.SeqCNN(fusion_matrices).view(self.batch_size, self.cdd_size, self.his_size, -1)     \n",
    "        fusion_vectors = torch.mean(fusion_vectors, dim=-2)\n",
    "        print(fusion_vectors.shape)\n",
    "        return fusion_vectors\n",
    "    \n",
    "    def _click_predictor(self,fusion_repr):\n",
    "        \"\"\" calculate batch of click probability              \n",
    "        Args:\n",
    "            fusion_repr: tensor of [batch_size, cdd_size, repr_dim]\n",
    "        \n",
    "        Returns:\n",
    "            score: tensor of [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = self.learningToRank(fusion_repr)\n",
    "\n",
    "        if self.cdd_size > 1:\n",
    "            score = nn.functional.log_softmax(score,dim=1)\n",
    "        else:\n",
    "            score = torch.sigmoid(score)\n",
    "        \n",
    "        return score.squeeze()\n",
    "\n",
    "    def forward(self,x):\n",
    "        cdd_news_embedding = self._news_encoder(x['candidate_title'].long().to(self.device))\n",
    "        his_news_embedding = self._news_encoder(x['clicked_title'].long().to(self.device))\n",
    "\n",
    "        fusion_vectors = self._fusion(cdd_news_embedding, his_news_embedding)   \n",
    "        score_batch = self._click_predictor(fusion_vectors)\n",
    "        return score_batch\n",
    "gcaModel = GCAModel(hparams, vocab=vocab).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = next(iter(loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcaModel(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(gcaModel, hparams, loader_train, loader_test, loader_validate, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}