{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnncondad67fb259925d4833a703b0467175fd55",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "3eb98a31bb4fe483f921d6d3a56a708e0ea8295072fddff1b0a8d949ab7fd102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import prepare,analyse,constructBasicDict,tailorData"
   ]
  },
  {
   "source": [
    "### Hyper parameters setting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'npratio':4,\n",
    "    'mode':'train',\n",
    "    'scale':'demo',\n",
    "    'batch_size':10,\n",
    "    'his_size':50,\n",
    "    'title_size':15,\n",
    "    'abs_size':20,\n",
    "    'device':'cpu',\n",
    "    'attrs': ['title'],\n",
    "    'k': 0,\n",
    "    'validate':False,\n",
    "    'onehot':True\n",
    "}\n",
    "# torch.cuda.set_device(hparams['device'])"
   ]
  },
  {
   "source": [
    "### Construct necessary dictionaries\n",
    "- already done and the results are in `data/dictionaries`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructBasicDict(attrs=['title'],path='/home/peitian_zhang/Data/MIND')"
   ]
  },
  {
   "source": [
    "### View data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, loaders = prepare(hparams, pin_memory=False)\n",
    "\n",
    "# for encoding news only\n",
    "# vocab, loaders = prepare(hparams, news=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_train\n",
    "a = next(iter(loaders[0]))\n",
    "# loader_dev\n",
    "b = next(iter(loaders[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['candidate_vert_onehot'].shape, b['clicked_ver']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tailor Data to demo size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailor 2000 impressions from MINDsmall_train to form MINDdemo_train\n",
    "tailorData('/home/peitian_zhang/Data/MIND/MINDsmall_train/behaviors.tsv',2000)\n",
    "\n",
    "tailorData('/home/peitian_zhang/Data/MIND/MINDsmall_dev/behaviors.tsv',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze MIND Datasets\n",
    "- average title length\n",
    "- average abstract length\n",
    "- average history length\n",
    "- average impression capacity\n",
    "- count of history exceeding 50\n",
    "- count of empty history\n",
    "- count of multi-clicked impressions "
   ]
  },
  {
   "source": [
    "analyse(hparams)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## The rest is for developing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "hparams = {\n",
    "    'npratio':4,\n",
    "    'mode':'train',\n",
    "    'scale':'demo',\n",
    "    'batch_size':2,\n",
    "    'his_size':50,\n",
    "    'title_size':15,\n",
    "    'abs_size':20,\n",
    "    'device':'cpu',\n",
    "    'attrs': ['title'],\n",
    "    'k': 20,\n",
    "    'validate':False,\n",
    "    'onehot':False\n",
    "}\n",
    "# torch.cuda.set_device(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "from utils.utils import newsample, getId2idx, tokenize, getVocab\n",
    "\n",
    "class MIND(Dataset):\n",
    "    \"\"\" Map Style Dataset for MIND, return positive samples with negative sampling when training, or return each sample when developing.\n",
    "\n",
    "    Args:\n",
    "        hparams(dict): pre-defined dictionary of hyper parameters\n",
    "        news_file(str): path of news_file\n",
    "        behaviors_file(str): path of behaviors_file\n",
    "        shuffle(bool): whether to shuffle the order of impressions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams, news_file, behaviors_file, shuffle_pos=False, validate=False):\n",
    "        # initiate the whole iterator\n",
    "        self.npratio = hparams['npratio']\n",
    "        self.shuffle_pos = shuffle_pos\n",
    "\n",
    "        self.news_file = news_file\n",
    "        self.behaviors_file = behaviors_file\n",
    "        self.col_spliter = '\\t'\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.title_size = hparams['title_size']\n",
    "        self.abs_size = hparams['abs_size']\n",
    "        self.his_size = hparams['his_size']\n",
    "\n",
    "        self.onehot = hparams['onehot']\n",
    "\n",
    "        if 'k' in hparams:\n",
    "            self.k = hparams['k']\n",
    "\n",
    "        self.mode = re.search(\n",
    "            '{}_(.*)/'.format(hparams['scale']), news_file).group(1)\n",
    "\n",
    "        # there are only two types of vocabulary\n",
    "        self.vocab = getVocab('data/dictionaries/vocab_whole.pkl')\n",
    "\n",
    "        self.nid2index = getId2idx(\n",
    "            'data/dictionaries/nid2idx_{}_{}.json'.format(hparams['scale'], self.mode))\n",
    "        self.uid2index = getId2idx(\n",
    "            'data/dictionaries/uid2idx_{}.json'.format(hparams['scale']))\n",
    "        self.vert2onehot = getId2idx(\n",
    "            'data/dictionaries/vert2onehot.json'\n",
    "        )\n",
    "        self.subvert2onehot = getId2idx(\n",
    "            'data/dictionaries/subvert2onehot.json'\n",
    "        )\n",
    "\n",
    "        if validate:\n",
    "            self.mode = 'dev'\n",
    "\n",
    "        self.init_news()\n",
    "        self.init_behaviors()\n",
    "\n",
    "    def init_news(self):\n",
    "        \"\"\"\n",
    "            init news information given news file, such as news_title_array.\n",
    "        \"\"\"\n",
    "\n",
    "        # VERY IMPORTANT!!! FIXME\n",
    "        # The nid2idx dictionary must follow the original order of news in news.tsv\n",
    "\n",
    "        titles = [[1]*self.title_size]\n",
    "        title_pad = [[self.title_size]]\n",
    "        abstracts = [[1]*self.abs_size]\n",
    "        abs_pad = [[self.abs_size]]\n",
    "\n",
    "        # pure text of the title\n",
    "        # titles = [['hello MIND']]\n",
    "        categories = [[1]]\n",
    "        subcategories = [[1]]\n",
    "\n",
    "        with open(self.news_file, \"r\", encoding='utf-8') as rd:\n",
    "            for idx in rd:\n",
    "                nid, vert, subvert, title, ab, url, _, _ = idx.strip(\"\\n\").split(self.col_spliter)\n",
    "\n",
    "                title_token = tokenize(title, self.vocab)\n",
    "                titles.append(title_token[:self.title_size] + [1] * (self.title_size - len(title_token)))\n",
    "                title_pad.append([max(self.title_size - len(title_token), 0)])\n",
    "\n",
    "                abs_token = tokenize(ab, self.vocab)\n",
    "                abstracts.append(abs_token[:self.abs_size] + [1] * (self.abs_size - len(abs_token)))\n",
    "                abs_pad.append([max(self.abs_size - len(abs_token), 0)])\n",
    "\n",
    "                categories.append(tokenize(vert, self.vocab))\n",
    "                subcategories.append(tokenize(subvert, self.vocab))\n",
    "\n",
    "        # self.titles = titles\n",
    "        self.news_title_array = np.asarray(titles)\n",
    "        self.title_pad = np.asarray(title_pad)\n",
    "        self.abs_array = np.asarray(abstracts)\n",
    "        self.abs_pad = np.asarray(abs_pad)\n",
    "        self.vert_array = np.asarray(categories)\n",
    "        self.subvert_array = np.asarray(subcategories)\n",
    "\n",
    "    def init_behaviors(self):\n",
    "        \"\"\"\n",
    "            init behavior logs given behaviors file.\n",
    "        \"\"\"\n",
    "        # list of list of history news index\n",
    "        self.histories = []\n",
    "        # list of user index\n",
    "        self.uindexes = []\n",
    "        # list of list of history padding length\n",
    "        self.his_pad = []\n",
    "        # list of impression indexes\n",
    "        # self.impr_indexes = []\n",
    "\n",
    "        # only store positive behavior\n",
    "        if self.mode == 'train':\n",
    "            # list of list of clicked candidate news index along with its impression index\n",
    "            self.imprs = []\n",
    "            # dictionary of list of unclicked candidate news index\n",
    "            self.negtives = {}\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding='utf-8') as rd:\n",
    "                for idx in rd:\n",
    "                    impr_index, uid, time, history, impr = idx.strip(\"\\n\").split(self.col_spliter)\n",
    "                    # important to subtract 1 because all list related to behaviors start from 0\n",
    "                    impr_index = int(impr_index) - 1\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "                    if self.k:\n",
    "                        # guarantee there are at least k history not masked\n",
    "                        self.his_pad.append(\n",
    "                            min(max(self.his_size - len(history), 0), self.his_size - self.k))\n",
    "                    else:\n",
    "                        self.his_pad.append(max(self.his_size - len(history), 0))\n",
    "\n",
    "                    # tailor user's history or pad 0\n",
    "                    history = history[:self.his_size] + [0] * (self.his_size - len(history))\n",
    "                    impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                    labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store negative samples of each impression\n",
    "                    negatives = []\n",
    "\n",
    "                    for news, label in zip(impr_news, labels):\n",
    "                        if label == 1:\n",
    "                            self.imprs.append((impr_index, news))\n",
    "                        else:\n",
    "                            negatives.append(news)\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    self.histories.append(history)\n",
    "                    self.negtives[impr_index] = negatives\n",
    "                    self.uindexes.append(uindex)\n",
    "\n",
    "        # store every behaviors\n",
    "        elif self.mode == 'dev':\n",
    "            # list of every candidate news index along with its impression index and label\n",
    "            self.imprs = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding='utf-8') as rd:\n",
    "                for idx in rd:\n",
    "                    impr_index, uid, time, history, impr = idx.strip(\"\\n\").split(self.col_spliter)\n",
    "                    impr_index = int(impr_index) - 1\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "                    if self.k:\n",
    "                        # guarantee there are at least k history not masked\n",
    "                        self.his_pad.append(\n",
    "                            min(max(self.his_size - len(history), 0), self.his_size - self.k))\n",
    "                    else:\n",
    "                        self.his_pad.append(max(self.his_size - len(history), 0))\n",
    "\n",
    "                    # tailor user's history or pad 0\n",
    "                    history = history[:self.his_size] + [0] * (self.his_size - len(history))\n",
    "                    impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                    labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store every impression\n",
    "                    for news, label in zip(impr_news, labels):\n",
    "                        self.imprs.append((impr_index, news, label))\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    self.histories.append(history)\n",
    "                    self.uindexes.append(uindex)\n",
    "\n",
    "        # store every behaviors\n",
    "        elif self.mode == 'test':\n",
    "            # list of every candidate news index along with its impression index and label\n",
    "            self.imprs = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding='utf-8') as rd:\n",
    "                for idx in rd:\n",
    "                    impr_index, uid, time, history, impr = idx.strip(\"\\n\").split(self.col_spliter)\n",
    "                    impr_index = int(impr_index) - 1\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "                    if self.k:\n",
    "                        # guarantee there are at least k history not masked\n",
    "                        self.his_pad.append(\n",
    "                            min(max(self.his_size - len(history), 0), self.his_size - self.k))\n",
    "                    else:\n",
    "                        self.his_pad.append(max(self.his_size - len(history), 0))\n",
    "\n",
    "                    # tailor user's history or pad 0\n",
    "                    history = history[:self.his_size] + [0] * (self.his_size - len(history))\n",
    "                    impr_news = [self.nid2index[i] for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store every impression\n",
    "                    for news in impr_news:\n",
    "                        self.imprs.append((impr_index, news))\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    self.histories.append(history)\n",
    "                    self.uindexes.append(uindex)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            return length of the whole dataset\n",
    "        \"\"\"\n",
    "        return len(self.imprs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\" return data\n",
    "        Args:\n",
    "            index: the index for stored impression\n",
    "\n",
    "        Returns:\n",
    "            back_dic: dictionary of data slice\n",
    "        \"\"\"\n",
    "\n",
    "        impr = self.imprs[index] # (impression_index, news_index)\n",
    "        impr_index = impr[0]\n",
    "        impr_news = impr[1]\n",
    "\n",
    "        user_index = [self.uindexes[impr_index]]\n",
    "\n",
    "        # each time called to return positive one sample and its negative samples\n",
    "        if self.mode == 'train':\n",
    "            # user's unclicked news in the same impression\n",
    "            negs = self.negtives[impr_index]\n",
    "            neg_list, neg_pad = newsample(negs, self.npratio)\n",
    "\n",
    "            cdd_ids = np.asarray([impr_news] + neg_list)\n",
    "            label = np.asarray([1] + [0] * self.npratio)\n",
    "\n",
    "            if self.shuffle_pos:\n",
    "                s = np.arange(0, len(label), 1)\n",
    "                np.random.shuffle(s)\n",
    "                cdd_ids = np.asarray(cdd_ids)[s]\n",
    "                label = np.asarray(label)[s]\n",
    "\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = np.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_ids = self.histories[impr_index]\n",
    "\n",
    "            # in case the user has no history records, do not mask\n",
    "            if self.his_pad[impr_index] == self.his_size or self.his_pad[impr_index] == 0:\n",
    "                his_mask = his_mask\n",
    "            else:\n",
    "                his_mask[-self.his_pad[impr_index]:] = [True]\n",
    "\n",
    "            # pad in candidate\n",
    "            # candidate_mask = [1] * neg_pad + [0] * (self.npratio + 1 - neg_pad)\n",
    "\n",
    "            # pad in title\n",
    "            candidate_title_pad = [(self.title_size - i[0])*[1] + i[0]*[0] for i in self.title_pad[cdd_ids]]\n",
    "            clicked_title_pad = [(self.title_size - i[0])*[1] + i[0]*[0] for i in self.title_pad[his_ids]]\n",
    "            candidate_abs_pad = [(self.abs_size - i[0])*[1] + i[0]*[0] for i in self.abs_pad[cdd_ids]]\n",
    "            clicked_abs_pad = [(self.abs_size - i[0])*[1] + i[0]*[0] for i in self.abs_pad[his_ids]]\n",
    "\n",
    "            candidate_title_index = self.news_title_array[cdd_ids]\n",
    "            clicked_title_index = self.news_title_array[his_ids]\n",
    "            candidate_abs_index = self.abs_array[cdd_ids]\n",
    "            clicked_abs_index = self.abs_array[his_ids]\n",
    "            candidate_vert_index = self.vert_array[cdd_ids]\n",
    "            clicked_vert_index = self.vert_array[his_ids]\n",
    "            candidate_subvert_index = self.subvert_array[cdd_ids]\n",
    "            clicked_subvert_index = self.subvert_array[his_ids]\n",
    "\n",
    "            back_dic = {\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                # \"cdd_mask\": np.asarray(neg_pad),\n",
    "                'cdd_id': cdd_ids,\n",
    "                \"candidate_title\": candidate_title_index,\n",
    "                \"candidate_title_pad\": np.asarray(candidate_title_pad),\n",
    "                \"candidate_abs\": candidate_abs_index,\n",
    "                \"candidate_abs_pad\": np.asarray(candidate_abs_pad),\n",
    "                \"candidate_vert\": candidate_vert_index,\n",
    "                \"candidate_subvert\": candidate_subvert_index,\n",
    "                'his_id': np.asarray(his_ids),\n",
    "                \"clicked_title\": clicked_title_index,\n",
    "                \"clicked_title_pad\": np.asarray(clicked_title_pad),\n",
    "                \"clicked_abs\": clicked_abs_index,\n",
    "                \"clicked_abs_pad\": np.asarray(clicked_abs_pad),\n",
    "                \"clicked_vert\": clicked_vert_index,\n",
    "                \"clicked_subvert\": clicked_subvert_index,\n",
    "                \"his_mask\": his_mask,\n",
    "                \"labels\": label\n",
    "            }\n",
    "\n",
    "            if self.onehot:\n",
    "                candidate_vert_onehot = [self.vert2onehot[str(i[0])] for i in candidate_vert_index]\n",
    "                clicked_vert_onehot = [self.vert2onehot[str(i[0])] for i in clicked_vert_index]\n",
    "\n",
    "                candidate_subvert_onehot = [self.subvert2onehot[str(i[0])] for i in candidate_subvert_index]\n",
    "                clicked_subvert_onehot = [self.subvert2onehot[str(i[0])] for i in clicked_subvert_index]\n",
    "\n",
    "                back_dic['candidate_vert_onehot'] = np.asarray(candidate_vert_onehot)\n",
    "                back_dic['clicked_vert_onehot'] = np.asarray(clicked_vert_onehot)\n",
    "                back_dic['candidate_subvert_onehot'] = np.asarray(candidate_subvert_onehot)\n",
    "                back_dic['clicked_subvert_onehot'] = np.asarray(clicked_subvert_onehot)\n",
    "\n",
    "            return back_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams['mode'] = 'dev'\n",
    "hparams['onehot'] = True\n",
    "path='/home/peitian_zhang/Data/MIND'\n",
    "news_file = path+'/MIND'+hparams['scale']+'_{}/news.tsv'.format(hparams['mode'])\n",
    "behavior_file = path+'/MIND' + hparams['scale']+'_{}/behaviors.tsv'.format(hparams['mode'])\n",
    "dataset = MIND(hparams=hparams, news_file=news_file,behaviors_file=behavior_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2news = {v:k for k,v in dataset.nid2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2news[13998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vocab['mmaufc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=5)\n",
    "list(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert2onehot = getId2idx(\n",
    "    'data/dictionaries/vert2onehot.json'\n",
    ")\n",
    "subvert2onehot = getId2idx(\n",
    "    'data/dictionaries/subvert2onehot.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvert2onehot['1'] = [0]*len(subvert2onehot['2513'])\n",
    "del subvert2onehot['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert2onehot['1'] = [0]*len(vert2onehot['138'])\n",
    "del vert2onehot['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert2onehot['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['candidate_vert_onehot'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(dataset, batch_size=10, pin_memory=False, num_workers=0, drop_last=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(loader_train))['candidate_vert'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "\n",
    "\n",
    "def news_token_generator(news_file_list, tokenizer, attrs):\n",
    "    ''' merge and deduplicate training news and testing news then iterate, collect attrs into a single sentence and generate it\n",
    "\n",
    "    Args:\n",
    "        tokenizer: torchtext.data.utils.tokenizer\n",
    "        attrs: list of attrs to be collected and yielded\n",
    "    Returns:\n",
    "        a generator over attrs in news\n",
    "    '''\n",
    "    news_df_list = []\n",
    "    for f in news_file_list:\n",
    "        news_df_list.append(pd.read_table(f, index_col=None, names=[\n",
    "                            'newsID', 'category', 'subcategory', 'title', 'abstract', 'url', 'entity_title', 'entity_abstract'], quoting=3))\n",
    "\n",
    "    news_df = pd.concat(news_df_list).drop_duplicates()\n",
    "    news_iterator = news_df.iterrows()\n",
    "\n",
    "    for _, i in news_iterator:\n",
    "        content = []\n",
    "        for attr in attrs:\n",
    "            content.append(i[attr])\n",
    "\n",
    "        yield tokenizer(' '.join(content))\n",
    "\n",
    "\n",
    "def constructVocab(news_file_list, attrs):\n",
    "    \"\"\"\n",
    "        Build field using torchtext for tokenization\n",
    "\n",
    "    Returns:\n",
    "        torchtext.vocabulary\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        news_token_generator(news_file_list, tokenizer, attrs))\n",
    "\n",
    "    output = open(\n",
    "        'data/dictionaries/vocab_{}.pkl'.format(','.join(attrs)), 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "attrs = ['title','category','subcategory']\n",
    "path= '/home/peitian_zhang/Data/MIND'\n",
    "scale = 'large'\n",
    "news_file_list = [path + '/MIND{}_train/news.tsv'.format(scale), path + '/MIND{}_dev/news.tsv'.format(scale), path + '/MIND{}_test/news.tsv'.format(scale)]\n",
    "constructVocab(news_file_list, attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subvert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in subvert:\n",
    "    if (tokenize(i, dataset.vocab)) == [0]:\n",
    "        print(\"fuck\")\n",
    "    if dataset.vocab[i] == 0:\n",
    "        print(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert,subvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['basketball_nba_videos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import getVocab,tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.itos[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvert2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.itos[771]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vert2onehot = {}\n",
    "for k,v in vert2idx.items():\n",
    "    a = np.zeros((len(vert2idx)))\n",
    "    index = np.asarray([v])\n",
    "    a[index] = 1\n",
    "    vert2onehot[int(k)] = a.tolist()\n",
    "vert2onehot[1] = [0]*len(vert2onehot[30])\n",
    "\n",
    "subvert2onehot = {}\n",
    "for k,v in subvert2idx.items():\n",
    "    a = np.zeros((len(subvert2idx)))\n",
    "    index = np.asarray([v])\n",
    "    a[index] = 1\n",
    "    subvert2onehot[int(k)] = a.tolist()\n",
    "subvert2onehot[1] = [0]*len(subvert2onehot[771])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvert2onehot[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(vert2onehot, open('data/dictionaries/vert2onehot.json','w'),ensure_ascii=False)\n",
    "json.dump(subvert2onehot, open('data/dictionaries/subvert2onehot.json','w'),ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}