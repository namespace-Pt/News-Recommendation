{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnncondad67fb259925d4833a703b0467175fd55",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "3eb98a31bb4fe483f921d6d3a56a708e0ea8295072fddff1b0a8d949ab7fd102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import prepare,analyse,constructBasicDict,tailorData\n",
    "from configs.ManualConfig import hparams"
   ]
  },
  {
   "source": [
    "### Construct necessary dictionaries\n",
    "- already done and the results are in `data/dictionaries`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructBasicDict(attrs=['title'],path='/home/peitian_zhang/Data/MIND')"
   ]
  },
  {
   "source": [
    "### View data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, loaders = prepare(hparams, pin_memory=False)\n",
    "\n",
    "# for encoding news only\n",
    "# vocab, loaders = prepare(hparams, news=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_train\n",
    "a = next(iter(loaders[0]))\n",
    "# loader_dev\n",
    "b = next(iter(loaders[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['candidate_title_pad'].shape, b['candidate_title_pad'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tailor Data to demo size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailor 2000 impressions from MINDsmall_train to form MINDdemo_train\n",
    "tailorData('/home/peitian_zhang/Data/MIND/MINDsmall_train/behaviors.tsv',2000)\n",
    "\n",
    "tailorData('/home/peitian_zhang/Data/MIND/MINDsmall_dev/behaviors.tsv',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze MIND Datasets\n",
    "- average title length\n",
    "- average abstract length\n",
    "- average history length\n",
    "- average impression capacity\n",
    "- count of history exceeding 50\n",
    "- count of empty history\n",
    "- count of multi-clicked impressions "
   ]
  },
  {
   "source": [
    "analyse(hparams)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## The rest is for developing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import prepare,analyse,constructBasicDict,tailorData\n",
    "from configs.ManualConfig import hparams\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "from utils.utils import newsample, getId2idx, tokenize, getVocab, my_collate\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MIND import MIND_impr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIND_impr(Dataset):\n",
    "    \"\"\" Map Style Dataset for MIND, return each impression once\n",
    "\n",
    "    Args:\n",
    "        hparams(dict): pre-defined dictionary of hyper parameters\n",
    "        news_file(str): path of news_file\n",
    "        behaviors_file(str): path of behaviors_file\n",
    "        shuffle(bool): whether to shuffle the order of impressions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams, news_file, behaviors_file, shuffle_pos=False, validate=False):\n",
    "        # initiate the whole iterator\n",
    "        self.npratio = hparams['npratio']\n",
    "        self.shuffle_pos = shuffle_pos\n",
    "\n",
    "        self.news_file = news_file\n",
    "        self.behaviors_file = behaviors_file\n",
    "        self.col_spliter = '\\t'\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.title_size = hparams['title_size']\n",
    "        self.abs_size = hparams['abs_size']\n",
    "        self.his_size = hparams['his_size']\n",
    "\n",
    "        self.onehot = hparams['onehot']\n",
    "        self.k = hparams['k']\n",
    "\n",
    "        # there are only two types of vocabulary\n",
    "        self.vocab = getVocab('data/dictionaries/vocab_whole.pkl')\n",
    "\n",
    "        self.nid2index = getId2idx(\n",
    "            'data/dictionaries/nid2idx_{}_{}.json'.format(hparams['scale'], 'dev'))\n",
    "        self.uid2index = getId2idx(\n",
    "            'data/dictionaries/uid2idx_{}.json'.format(hparams['scale']))\n",
    "        self.vert2onehot = getId2idx(\n",
    "            'data/dictionaries/vert2onehot.json'\n",
    "        )\n",
    "        self.subvert2onehot = getId2idx(\n",
    "            'data/dictionaries/subvert2onehot.json'\n",
    "        )\n",
    "\n",
    "        self.mode = 'dev'\n",
    "\n",
    "        self.init_news()\n",
    "        self.init_behaviors()\n",
    "\n",
    "    def init_news(self):\n",
    "        \"\"\"\n",
    "            init news information given news file, such as news_title_array.\n",
    "        \"\"\"\n",
    "\n",
    "        # VERY IMPORTANT!!! FIXME\n",
    "        # The nid2idx dictionary must follow the original order of news in news.tsv\n",
    "\n",
    "        titles = [[1]*self.title_size]\n",
    "        title_pad = [[self.title_size]]\n",
    "\n",
    "        with open(self.news_file, \"r\", encoding='utf-8') as rd:\n",
    "            for idx in rd:\n",
    "                nid, vert, subvert, title, ab, url, _, _ = idx.strip(\"\\n\").split(self.col_spliter)\n",
    "\n",
    "                title_token = tokenize(title, self.vocab)\n",
    "                titles.append(title_token[:self.title_size] + [1] * (self.title_size - len(title_token)))\n",
    "                title_pad.append([max(self.title_size - len(title_token), 0)])\n",
    "\n",
    "        # self.titles = titles\n",
    "        self.news_title_array = np.asarray(titles)\n",
    "        self.title_pad = np.asarray(title_pad)\n",
    "\n",
    "    def init_behaviors(self):\n",
    "        \"\"\"\n",
    "            init behavior logs given behaviors file.\n",
    "        \"\"\"\n",
    "        # list of list of history news index\n",
    "        self.histories = []\n",
    "        # list of user index\n",
    "        self.uindexes = []\n",
    "        # list of list of history padding length\n",
    "        self.his_pad = []\n",
    "        # list of impression indexes\n",
    "        # self.impr_indexes = []\n",
    "\n",
    "        # list of every candidate news index along with its impression index and label\n",
    "        self.imprs = []\n",
    "\n",
    "        with open(self.behaviors_file, \"r\", encoding='utf-8') as rd:\n",
    "            for idx in rd:\n",
    "                impr_index, uid, time, history, impr = idx.strip(\"\\n\").split(self.col_spliter)\n",
    "                impr_index = int(impr_index) - 1\n",
    "\n",
    "                history = [self.nid2index[i] for i in history.split()]\n",
    "                # tailor user's history or pad 0\n",
    "                history = history[:self.his_size] + [0] * (self.his_size - len(history))\n",
    "                impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "                # user will always in uid2index\n",
    "                uindex = self.uid2index[uid]\n",
    "\n",
    "                # store every impression\n",
    "                self.imprs.append((impr_index, impr_news[0], labels[0]))\n",
    "\n",
    "                # 1 impression correspond to 1 of each of the following properties\n",
    "                self.histories.append(history)\n",
    "                self.uindexes.append(uindex)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            return length of the whole dataset\n",
    "        \"\"\"\n",
    "        return len(self.imprs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\" return data\n",
    "        Args:\n",
    "            index: the index for stored impression\n",
    "\n",
    "        Returns:\n",
    "            back_dic: dictionary of data slice\n",
    "        \"\"\"\n",
    "\n",
    "        impr = self.imprs[index] # (impression_index, news_index)\n",
    "        impr_index = impr[0]\n",
    "        impr_news = impr[1]\n",
    "\n",
    "\n",
    "        user_index = [self.uindexes[impr_index]]\n",
    "\n",
    "        cdd_ids = [impr_news]\n",
    "\n",
    "        his_ids = self.histories[impr_index]\n",
    "\n",
    "        user_index = [self.uindexes[impr_index]]\n",
    "        label = impr[2]\n",
    "\n",
    "        candidate_title_index = [self.news_title_array[impr_news]]\n",
    "        clicked_title_index = self.news_title_array[his_ids]\n",
    "        back_dic = {\n",
    "            \"impression_index\": impr_index + 1,\n",
    "            \"user_index\": np.asarray(user_index),\n",
    "            'cdd_id': np.asarray(cdd_ids),\n",
    "            \"candidate_title\": np.asarray(candidate_title_index),\n",
    "            'his_id': np.asarray(his_ids),\n",
    "            \"clicked_title\": clicked_title_index,\n",
    "            \"labels\": np.asarray([label])\n",
    "        }\n",
    "\n",
    "        return back_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['mode'] = 'dev'\n",
    "# hparams['onehot'] = True\n",
    "path='/home/peitian_zhang/Data/MIND'\n",
    "news_file = path+'/MIND'+hparams['scale']+'_{}/news.tsv'.format(hparams['mode'])\n",
    "behavior_file = path+'/MIND' + hparams['scale']+'_{}/behaviors.tsv'.format(hparams['mode'])\n",
    "dataset = MIND_impr(hparams=hparams, news_file=news_file,behaviors_file=behavior_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'impression_index': [1, 2, 3, 4, 5],\n",
       " 'user_index': tensor([[1929],\n",
       "         [1930],\n",
       "         [1931],\n",
       "         [1932],\n",
       "         [1933]]),\n",
       " 'cdd_id': tensor([[36180],\n",
       "         [35201],\n",
       "         [39669],\n",
       "         [  438],\n",
       "         [39227]]),\n",
       " 'candidate_title': tensor([[[  255,  9098,     5,  1290,  1482,     6,   314,  1087,   851,     9,\n",
       "            3812,  7065,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
       " \n",
       "         [[  594,    81,   264,   229,   561,    29,  2688,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
       " \n",
       "         [[  206,   634,   391,   213,   240,     7,   732,    10,   638,  1702,\n",
       "              67,   132,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
       " \n",
       "         [[  106, 47085,    18,  1206,  1216,   106,  1965,  1514,    42,   138,\n",
       "            2443,  1223,    16, 17795,    43,   366,   144,  1622,   158,     1]],\n",
       " \n",
       "         [[ 1479,  2277,    17,  2880,  1404,  6455,  3228,     8,  2377,  1066,\n",
       "              18, 40868,  8148,     1,     1,     1,     1,     1,     1,     1]]]),\n",
       " 'his_id': tensor([[ 7499, 34171, 15796, 32731, 32139, 18228,  1736, 30270, 35479, 30833,\n",
       "          12951, 26650, 30302,  1692, 36210,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [32076, 20334, 26207, 21430, 19361, 29087, 16136,  9820, 14253, 20961,\n",
       "          25394,  1692, 31358,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [16846, 19564,  7499, 12258, 35215, 19081, 26338, 26568, 18162,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [ 5438, 35294, 19463, 17541, 25711, 12364, 31031,  8638, 20165, 23884,\n",
       "          15978, 26338, 21539, 17837, 10227, 14044, 24008, 20999, 15451,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [27725, 31370, 35302,   126, 23483,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'clicked_title': tensor([[[ 3432,    11,  4474,  ...,     1,     1,     1],\n",
       "          [  675,   567,   610,  ...,     1,     1,     1],\n",
       "          [ 5567,  5094,  1056,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[  375,  2641,    16,  ...,     1,     1,     1],\n",
       "          [ 4685,  1201,  1217,  ...,     1,     1,     1],\n",
       "          [ 5269,     5,   670,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[  128,   977,     6,  ...,     1,     1,     1],\n",
       "          [   15,   190,   659,  ...,     1,     1,     1],\n",
       "          [ 3432,    11,  4474,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[   43,  1613,   832,  ...,   174,    43,   300],\n",
       "          [  692,    19,     8,  ...,     1,     1,     1],\n",
       "          [  269,   174,     8,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[ 1642, 13311,  5581,  ...,     1,     1,     1],\n",
       "          [  128,    44,   291,  ...,     1,     1,     1],\n",
       "          [ 4023,    94,  2081,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1],\n",
       "          [    1,     1,     1,  ...,     1,     1,     1]]]),\n",
       " 'labels': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=5, collate_fn=my_collate)\n",
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}