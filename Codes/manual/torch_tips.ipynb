{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bit4c03300bedca44f8b0013abe02048abc",
   "display_name": "Python 3.7.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## torch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `broadcast`\n",
    "- Docs about this feature is concise\n",
    "- cannot handle dimension size larger than 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0.2157, 0.0024, 0.3610, 0.3878, 0.8161]]),\n",
       " tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]),\n",
       " tensor([[1.2157, 1.0024, 1.3610, 1.3878, 1.8161],\n",
       "         [1.2157, 1.0024, 1.3610, 1.3878, 1.8161],\n",
       "         [1.2157, 1.0024, 1.3610, 1.3878, 1.8161],\n",
       "         [1.2157, 1.0024, 1.3610, 1.3878, 1.8161],\n",
       "         [1.2157, 1.0024, 1.3610, 1.3878, 1.8161],\n",
       "         [1.2157, 1.0024, 1.3610, 1.3878, 1.8161]]))"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((1,5))\n",
    "b = torch.ones((6,5))\n",
    "a,b,a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `repeat_interleave`\n",
    "repeating samples along axis\n",
    "\n",
    "- equal to *cat and view*\n",
    "- *broadcast* a tensor when the dimension is not 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[1, 2]],\n\n        [[2, 3]]]) torch.Size([2, 1, 2])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[1, 2]],\n",
       " \n",
       "         [[1, 2]],\n",
       " \n",
       "         [[2, 3]],\n",
       " \n",
       "         [[2, 3]]]),\n",
       " tensor([[[1, 2]],\n",
       " \n",
       "         [[1, 2]],\n",
       " \n",
       "         [[2, 3]],\n",
       " \n",
       "         [[2, 3]]]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a =torch.tensor([[[1,2]],[[2,3]]])\n",
    "print(a,a.shape)\n",
    "\n",
    "a.repeat_interleave(repeats=2,dim=0),torch.cat([a.unsqueeze(dim=1)]*2,dim=1).view(-1,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `permute`\n",
    "\n",
    "let's suppose there is a tensor of $[dim0, dim1, dim2, dim3]$, then we permute it to $[dim3, dim1, dim2, dim0]$\n",
    "\n",
    "- **consequence**: origin value at $[a, b, c, d]$ will be switched to $[d, b, c, a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros([5,4,3,2]),\n",
    "# say a=2, b=3, c=1, d=1,\n",
    "a[2,3,1,1] = 1,\n",
    "print(\"origin 1 at [2,3,1,1]: {}\".format(a[2,3,1,1])),\n",
    "b = a.permute(3,1,2,0),\n",
    "print(\"permuted 1 at [3,1,2,0]: {}\".format(b[1,3,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nonzero`\n",
    "finding non-zero indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros(2,2)\n",
    "a[1,0] = 1\n",
    "a[1,1] = 1\n",
    "a.nonzero()"
   ]
  },
  {
   "source": [
    "### `matmul`\n",
    "tensor multiplication\n",
    "\n",
    "- Docs about this function is concise, I want to give an example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[2, 3, 4],\n",
       "          [0, 1, 0]]]),\n",
       " torch.Size([2, 2, 3]),\n",
       " tensor([[[1, 0],\n",
       "          [0, 1],\n",
       "          [1, 1]],\n",
       " \n",
       "         [[1, 1],\n",
       "          [0, 0],\n",
       "          [0, 1]]]),\n",
       " torch.Size([2, 3, 2]),\n",
       " tensor([[1, 0],\n",
       "         [0, 1],\n",
       "         [1, 1]]),\n",
       " torch.Size([3, 2]),\n",
       " tensor([[[4, 5],\n",
       "          [1, 0]],\n",
       " \n",
       "         [[6, 7],\n",
       "          [0, 1]]]),\n",
       " tensor([[[4, 5],\n",
       "          [1, 0]],\n",
       " \n",
       "         [[2, 6],\n",
       "          [0, 0]]]),\n",
       " tensor([[[4, 5],\n",
       "          [1, 0]],\n",
       " \n",
       "         [[1, 4],\n",
       "          [1, 1]]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[1,2,3],[1,0,0]],[[2,3,4],[0,1,0]]])\n",
    "b = torch.tensor([[[1,0],[0,1],[1,1]],[[1,1],[0,0],[0,1]]])\n",
    "\n",
    "a,a.shape, b, b.shape, b[0], b[0].shape,torch.matmul(a,b[0]), torch.matmul(a,b), torch.matmul(a[0],b), a[0].shape"
   ]
  },
  {
   "source": [
    "## torch.nn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `indice embedding`\n",
    "\n",
    "sometimes we have to create an embedding layer (*loop up layer*). The derivation from emebdding layer is straight forward: the last |n-1| dimension in embedding layer will be appended to the index tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 dimensional embedding:tensor([[[0.4890, 0.8069, 0.8093],\n         [0.6145, 0.3115, 0.6036]],\n\n        [[0.3977, 0.5009, 0.9692],\n         [0.8422, 0.7311, 0.7406]]]) of size torch.Size([2, 2, 3])\n\n2 dimensional embedding:tensor([[[[4.3572e-01, 3.7110e-01, 5.5503e-01],\n          [3.4236e-01, 3.0673e-01, 7.1331e-01],\n          [2.3374e-01, 3.6648e-01, 9.3444e-01],\n          [4.8977e-01, 9.6126e-01, 5.6057e-02],\n          [5.8815e-01, 6.7106e-01, 9.9311e-01]],\n\n         [[5.7527e-01, 2.8838e-01, 8.5722e-01],\n          [5.2984e-01, 8.5100e-02, 5.7482e-01],\n          [7.2258e-01, 2.0804e-01, 7.0520e-02],\n          [9.9510e-01, 8.3778e-01, 6.3597e-01],\n          [5.6489e-01, 3.9864e-01, 6.1766e-01]]],\n\n\n        [[[5.9626e-01, 9.5241e-02, 8.3267e-01],\n          [1.1131e-01, 3.0026e-01, 6.5316e-01],\n          [6.7256e-01, 4.0614e-01, 6.9878e-01],\n          [7.8866e-01, 7.1249e-01, 7.5015e-01],\n          [2.0895e-01, 5.7120e-01, 8.2893e-01]],\n\n         [[7.8303e-04, 1.8824e-01, 4.3864e-01],\n          [3.3937e-01, 3.7026e-01, 9.4996e-01],\n          [9.8147e-01, 7.1041e-01, 6.8880e-01],\n          [8.0600e-01, 2.4338e-01, 9.8605e-02],\n          [8.7597e-01, 7.4632e-02, 4.3825e-01]]]]) of size torch.Size([2, 2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embedding_layer = torch.rand((5,3))\n",
    "index_tensor = torch.tensor([[3,4],[0,1]]) # only tensor of dtype=torch.long works\n",
    "print(\"1 dimensional embedding:{} of size {}\\n\".format(embedding_layer[index_tensor], embedding_layer[index_tensor].shape))\n",
    "\n",
    "embedding_layer = torch.rand((5,5,3))\n",
    "print(\"2 dimensional embedding:{} of size {}\".format(embedding_layer[index_tensor], embedding_layer[index_tensor].shape))"
   ]
  },
  {
   "source": [
    "### `nn.CosineSimilarity`\n",
    "`PyTorch` provides convenient api for computing cosine similarity between two tensor, however it's confusing when dimension is more than one.\n",
    "\n",
    "- From my perspective, the `dim` parameter can be viewed as the dimension to *compress*, which means computing cosine similarity along `dim` is actually transforming the vector on this dimension to a single value.\n",
    "- As for calculating, we first slice the tensor of given `dim` and compute cosine similarity pair-wise\n",
    "- when `dim` is higher dimension:\n",
    "    - `dim` = $n$, $n>=2$: value at the same place across the given dimension will be packed into a vector\n",
    "    - `dim` = -1: value at the last dimension will be collected into a vector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0.7652, 0.9477],\n",
       "         [0.9658, 0.5803],\n",
       "         [0.6498, 0.9634]]),\n",
       " tensor([0.7652, 0.9477]),\n",
       " tensor([0.7652, 0.9658, 0.6498]),\n",
       " tensor([0.9477, 0.5803, 0.9634]))"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "\" example for cosine similarity along the last dimension \"\n",
    "cos = CosineSimilarity(dim=2)\n",
    "\n",
    "a = torch.rand((3,2,3))\n",
    "b = torch.rand((3,2,3))\n",
    "\n",
    "c = a[0]\n",
    "d = b[0]\n",
    "\n",
    "e = a[:,0,:].unsqueeze(dim=1)\n",
    "f = b[:,0,:].unsqueeze(dim=2)\n",
    "g = a[:,1,:].unsqueeze(dim=1)\n",
    "h = b[:,1,:].unsqueeze(dim=2)\n",
    "\n",
    "result_1 = torch.matmul(e,f) / torch.sqrt(torch.matmul(e,e.permute(0,2,1)) * torch.matmul(f.permute(0,2,1),f))\n",
    "result_2 = torch.matmul(g,h) / torch.sqrt(torch.matmul(g,g.permute(0,2,1)) * torch.matmul(h.permute(0,2,1),h))\n",
    "\n",
    "cos_2 = CosineSimilarity(dim=1)\n",
    "cos(a,b), cos_2(c,d), result_1.squeeze(), result_2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0., 0.]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.]]),\n",
       " tensor([[0.8044, 0.7442],\n",
       "         [0.1688, 0.2175]]),\n",
       " tensor([0., 0.]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "cos = CosineSimilarity(dim=-1)\n",
    "a = torch.zeros((1,2))\n",
    "b = torch.zeros((2,2))\n",
    "c = torch.rand(2,2)\n",
    "a,b,c,cos(a,b),cos(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 3.]]]),\n",
       " tensor([[[ 1.,  2.,  3.],\n",
       "          [ 2.,  3.,  4.]],\n",
       " \n",
       "         [[ 5.,  6.,  9.],\n",
       "          [-9., -2., -7.]]]),\n",
       " tensor([[ 1.0000,  0.9926],\n",
       "         [ 0.9868, -0.7850]]))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\" broadcast in CosineSimilarity \"\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "from utils.TestTensors import t_2_2_3\n",
    "\n",
    "cos = CosineSimilarity(dim=-1)\n",
    "a = torch.tensor([[[1,2,3.]]])\n",
    "a, t_2_2_3, cos(a,t_2_2_3)"
   ]
  },
  {
   "source": [
    "### `nn.LayerNorm`\n",
    "\n",
    "Layer Normalization is applied over the last given dimensions of the input tensor, i.e. `mean` and `variance` are calculated within the current input, rather than the whole batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[-0.5000,  0.5000,  0.0000],\n",
       "          [ 0.0000,  1.0000, -1.0000]],\n",
       " \n",
       "         [[ 1.0000,  2.0000,  3.0000],\n",
       "          [ 5.0000,  9.0000, 11.0000]]]),\n",
       " tensor([[[-0.7746,  0.7746,  0.0000],\n",
       "          [ 0.0000,  1.5492, -1.5492]],\n",
       " \n",
       "         [[-1.1352, -0.8627, -0.5903],\n",
       "          [-0.0454,  1.0444,  1.5893]]], grad_fn=<NativeLayerNormBackward>))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "LayerNorm = torch.nn.LayerNorm((2,3))\n",
    "a = torch.tensor([[[-0.5,0.5,0],[0,1,-1]],[[1,2,3],[5,9,11]]])\n",
    "a,LayerNorm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd"
   ]
  },
  {
   "source": [
    "Only when the operation in the forward phrase is **not differentiable** while you want the gradient to be pass through that you should rewrite torch.autograd, where you can define your own backward algorithm to give an approximate gradient of the **indifferentiable** operation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `detach` \n",
    "\n",
    "*Straight-through trick*\n",
    "\n",
    "- transform a tensor while keep its gradient unchanged"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[2.0000e-04, 4.0000e+01]], requires_grad=True),\n",
       " tensor([[ 0., 40.]], grad_fn=<AddBackward0>))"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[0.0002,40.]],requires_grad=True)\n",
    "b = torch.tensor([[0.,40.]]) - a.detach() + a\n",
    "a,b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}