{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".ipynb",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnncondad67fb259925d4833a703b0467175fd55",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "3eb98a31bb4fe483f921d6d3a56a708e0ea8295072fddff1b0a8d949ab7fd102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import train,prepare,evaluate,tune\n",
    "from models.Interactors import FIM_Interactor, KNRM_Interactor\n",
    "from models.Encoders.FIM import FIM_Encoder\n",
    "from models.SFI import SFI_gating, SFI_gating_MultiView\n",
    "from configs.ManualConfig import hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='sfi'\n",
    "hparams['k'] = 30\n",
    "hparams['his_size'] = 50\n",
    "hparams['select'] = 'gating'\n",
    "hparams['onehot'] = True\n",
    "hparams['device'] = 'cuda:0'\n",
    "# hparams['threshold'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-04-21 16:26:19,548] INFO (root) Hyper Parameters are\n",
      "{'scale': 'demo', 'mode': 'train', 'batch_size': 10, 'title_size': 20, 'abs_size': 40, 'his_size': 50, 'vert_num': 18, 'subvert_num': 293, 'npratio': 4, 'dropout_p': 0.2, 'query_dim': 200, 'embedding_dim': 300, 'filter_num': 150, 'value_dim': 16, 'head_num': 16, 'epochs': 8, 'metrics': 'auc,mean_mrr,ndcg@5,ndcg@10', 'device': 'cuda:0', 'attrs': ['title'], 'k': 30, 'select': 'gating', 'save_step': [0], 'news_id': False, 'validate': False, 'interval': 10, 'spadam': True, 'onehot': True, 'val_freq': 1, 'schedule': None}\n",
      "[2021-04-21 16:26:19,550] INFO (root) preparing dataset...\n",
      "[2021-04-21 16:26:23,267] INFO (torchtext.vocab) Loading vectors from .vector_cache/glove.840B.300d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "# hparams['validate'] = True\n",
    "vocab, loaders = prepare(hparams, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = next(iter(loaders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math,random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.Interactors import FIM_Interactor\n",
    "from models.Attention import Attention\n",
    "\n",
    "\n",
    "class SFI_gating(nn.Module):\n",
    "    def __init__(self, hparams, encoder, interactor=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cdd_size = (hparams['npratio'] +\n",
    "                         1) if hparams['npratio'] > 0 else 1\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.his_size = hparams['his_size']\n",
    "        self.signal_length = hparams['title_size']\n",
    "\n",
    "        self.k = hparams['k']\n",
    "\n",
    "        # contrasive learning deprecated\n",
    "        self.contra_num = 0\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.level = encoder.level\n",
    "        self.hidden_dim = encoder.hidden_dim\n",
    "\n",
    "        # concatenate category embedding and subcategory embedding\n",
    "\n",
    "        self.device = hparams['device']\n",
    "        # elements in the slice along dim will sum up to 1\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        if not interactor:\n",
    "            self.interactor = FIM_Interactor(self.level)\n",
    "        else:\n",
    "            self.interactor = interactor\n",
    "\n",
    "        final_dim = int(int(self.k / 3) /3) * int(int(self.signal_length / 3) / 3)**2 * 16\n",
    "        self.learningToRank = nn.Linear(final_dim, 1)\n",
    "\n",
    "        self.name = '-'.join(['sfi-gating', self.encoder.name, self.interactor.name])\n",
    "\n",
    "        # self.src = torch.ones(self.batch_size, self.cdd_size, self.k, self.his_size,device=self.device)\n",
    "        # self.dest = torch.zeros(self.batch_size, self.cdd_size, self.k, self.his_size,device=self.device)\n",
    "        # self.selectionProject = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        # self.selectionProject = nn.Sequential(\n",
    "        #     nn.Linear(self.hidden_dim, 128),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(128,100)\n",
    "        # )\n",
    "\n",
    "        # self.lstm = nn.LSTM(self.hidden_dim, self.hidden_dim//2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        if hasattr(self,'selectionProject'):\n",
    "            if isinstance(self.selectionProject, nn.Linear):\n",
    "                    nn.init.xavier_normal_(self.selectionProject.weight)\n",
    "            else:\n",
    "                for param in self.selectionProject:\n",
    "                    if isinstance(param, nn.Linear):\n",
    "                        nn.init.xavier_normal_(param.weight)\n",
    "\n",
    "        nn.init.xavier_normal_(self.learningToRank.weight)\n",
    "\n",
    "        if 'threshold' in hparams and hparams['threshold']:\n",
    "            threshold = torch.tensor([hparams['threshold']])\n",
    "            self.register_buffer('threshold', threshold)\n",
    "            if self.k != self.his_size:\n",
    "                raise ValueError(\"K value not matched!\")\n",
    "\n",
    "    def _news_attention(self, cdd_repr, his_repr, his_embedding, his_mask):\n",
    "        \"\"\" apply news-level attention\n",
    "\n",
    "        Args:\n",
    "            cdd_repr: tensor of [batch_size, cdd_size, hidden_dim]\n",
    "            his_repr: tensor of [batch_size, his_size, hidden_dim]\n",
    "            his_embedding: tensor of [batch_size, his_size, signal_length, level, *]\n",
    "            his_mask: tensor of [batch_size, his_size, 1]\n",
    "\n",
    "        Returns:\n",
    "            his_activated: tensor of [batch_size, cdd_size, k, signal_length, *]\n",
    "            his_focus: tensor of [batch_size, cdd_size, k, his_size]\n",
    "            pos_repr: tensor of [batch_size, cdd_size, contra_num, hidden_dim]\n",
    "            neg_repr: tensor of [batch_size, cdd_size, contra_num, hidden_dim]\n",
    "        \"\"\"\n",
    "        # [bs, cs, hs]\n",
    "        if hasattr(self, 'threshold'):\n",
    "            attn_weights = F.normalize(self.selectionProject(cdd_repr), dim=-1).matmul(F.normalize(self.selectionProject(his_repr),dim=-1).transpose(-2,-1))\n",
    "            # attn_weights = F.normalize(cdd_repr, dim=-1).matmul(F.normalize(his_repr, dim=-1).transpose(-1, -2))\n",
    "\n",
    "            his_activated = his_embedding.unsqueeze(dim=1) * (attn_weights.masked_fill(attn_weights<self.threshold, 0).view(self.batch_size, self.cdd_size, self.k, 1, 1, 1))\n",
    "\n",
    "            output = (his_activated, None)\n",
    "\n",
    "        else:\n",
    "            # cdd_repr = self.selectionProject(cdd_repr)\n",
    "            # his_repr = self.selectionProject(his_repr)\n",
    "\n",
    "            attn_weights = cdd_repr.matmul(his_repr.transpose(-1, -2))\n",
    "\n",
    "            # Masking off these 0s will force the gumbel_softmax to attend to only non-zero histories.\n",
    "            # Masking in candidate also cause such a problem, however we donot need to fix it\n",
    "\n",
    "            attn_weights = self.softmax(attn_weights.masked_fill(his_mask.transpose(-1, -2), -float(\"inf\")))\n",
    "            \n",
    "\n",
    "            # attn_weights = self.softmax(attn_weights)\n",
    "\n",
    "            _, attn_weights_sorted = attn_weights.detach().sort(dim=-1, descending=True)\n",
    "\n",
    "            # use scatter to map the index tensor to one-hot encoding, this is faster than F.one_hot\n",
    "            attn_focus = torch.zeros(self.batch_size, self.cdd_size, self.k, self.his_size,device=self.device)\n",
    "            src = torch.ones(self.batch_size, self.cdd_size, self.k, self.his_size,device=self.device)\n",
    "            his_focus = attn_focus.scatter(-1, attn_weights_sorted[:, :, :self.k].unsqueeze(dim=-1), src)\n",
    "\n",
    "            # [bs, cs, k, sl, level, fn]\n",
    "            his_activated = torch.matmul(his_focus, his_embedding.reshape(\n",
    "                self.batch_size, 1, self.his_size, -1)).view(self.batch_size, self.cdd_size, self.k, -1, self.level, self.hidden_dim)\n",
    "\n",
    "            output = (his_activated, his_focus)\n",
    "        return output\n",
    "\n",
    "    def _click_predictor(self, fusion_tensors):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            fusion_tensors: tensor of [batch_size, cdd_size, *]\n",
    "\n",
    "        Returns:\n",
    "            score: tensor of [batch_size, cdd_size], which is normalized click probabilty\n",
    "        \"\"\"\n",
    "        score = self.learningToRank(fusion_tensors).squeeze(dim=-1)\n",
    "        return score\n",
    "\n",
    "    def forward_(self, x):\n",
    "        if x['candidate_title'].shape[0] != self.batch_size:\n",
    "            self.batch_size = x['candidate_title'].shape[0]\n",
    "\n",
    "        cdd_news = x['candidate_title'].long().to(self.device)\n",
    "        cdd_news_embedding, cdd_news_repr, cdd_news_repr_selection = self.encoder(\n",
    "            cdd_news,\n",
    "            user_index=x['user_index'].long().to(self.device),\n",
    "            news_id=x['cdd_id'].long().to(self.device),\n",
    "            attn_mask=x['candidate_title_pad'].to(self.device))\n",
    "\n",
    "        his_news = x['clicked_title'].long().to(self.device)\n",
    "        his_news_embedding, his_news_repr, his_news_repr_selection = self.encoder(\n",
    "            his_news,\n",
    "            user_index=x['user_index'].long().to(self.device),\n",
    "            news_id=x['his_id'].long().to(self.device),\n",
    "            attn_mask=x['clicked_title_pad'].to(self.device))\n",
    "\n",
    "        output = self._news_attention(\n",
    "            cdd_news_repr_selection, his_news_repr_selection, his_news_embedding, x['his_mask'].to(self.device))\n",
    "\n",
    "        if self.interactor.name == 'knrm':\n",
    "            cdd_pad = x['candidate_title_pad'].float().to(self.device).view(self.batch_size, self.cdd_size, 1, 1, -1, 1)\n",
    "            if output[1] is not None:\n",
    "                his_pad = torch.matmul(output[1], x['clicked_title_pad'].float().to(self.device).reshape(\n",
    "                    self.batch_size, 1, self.his_size, -1)).view(self.batch_size, self.cdd_size, self.k, 1, 1, -1, 1)\n",
    "            else:\n",
    "                his_pad = x['clicked_title_pad'].float().to(self.device).view(self.batch_size, 1, self.k, 1, 1, self.signal_length, 1).expand(self.batch_size, self.cdd_size, self.k, 1, 1, self.signal_length, 1)\n",
    "\n",
    "            fusion_tensors = self.interactor(cdd_news_embedding, output[0], cdd_pad=cdd_pad, his_pad=his_pad)\n",
    "\n",
    "        elif self.interactor.name == 'fim':\n",
    "            fusion_tensors = self.interactor(cdd_news_embedding, output[0])\n",
    "\n",
    "        return self._click_predictor(fusion_tensors)\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = self.forward_(x)\n",
    "        if self.cdd_size > 1:\n",
    "            score = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            score = torch.sigmoid(score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIM_Encoder(nn.Module):\n",
    "    def __init__(self, hparams, vocab):\n",
    "        super().__init__()\n",
    "        self.name = 'fim'\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        # self.level = 3\n",
    "        self.level = 4\n",
    "\n",
    "        # concatenate category embedding and subcategory embedding\n",
    "        self.hidden_dim = hparams['filter_num']\n",
    "        self.embedding_dim = hparams['embedding_dim']\n",
    "\n",
    "        # pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors,sparse=True,freeze=False)\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_dim)\n",
    "        self.DropOut = nn.Dropout(p=hparams['dropout_p'])\n",
    "\n",
    "        self.query_words = nn.Parameter(torch.randn(\n",
    "            (1, self.hidden_dim), requires_grad=True))\n",
    "        self.query_levels = nn.Parameter(torch.randn(\n",
    "            (1, self.hidden_dim), requires_grad=True))\n",
    "\n",
    "        self.CNN_d1 = nn.Conv1d(in_channels=self.embedding_dim, out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size, dilation=1, padding=1)\n",
    "        self.CNN_d2 = nn.Conv1d(in_channels=self.embedding_dim, out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size, dilation=2, padding=2)\n",
    "        self.CNN_d3 = nn.Conv1d(in_channels=self.embedding_dim, out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size, dilation=3, padding=3)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
    "        self.selectionProject = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "\n",
    "        self.device = hparams['device']\n",
    "        self.attrs = hparams['attrs']\n",
    "\n",
    "        nn.init.xavier_normal_(self.CNN_d1.weight)\n",
    "        nn.init.xavier_normal_(self.CNN_d2.weight)\n",
    "        nn.init.xavier_normal_(self.CNN_d3.weight)\n",
    "        nn.init.xavier_normal_(self.selectionProject.weight)\n",
    "\n",
    "        for i in self.lstm.all_weights:\n",
    "            for j in i:\n",
    "                if len(j.size()) > 1:\n",
    "                    nn.init.orthogonal_(j)\n",
    "\n",
    "    def _HDC(self, news_embedding_set):\n",
    "        \"\"\" stack 1d CNN with dilation rate expanding from 1 to 3\n",
    "\n",
    "        Args:\n",
    "            news_embedding_set: tensor of [set_size, signal_length, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            news_embedding_dilations: tensor of [set_size, signal_length, levels(3), filter_num]\n",
    "        \"\"\"\n",
    "\n",
    "        # don't know what d_0 meant in the original paper\n",
    "        news_embedding_dilations = torch.zeros(\n",
    "            (news_embedding_set.shape[0], news_embedding_set.shape[1], 3, self.hidden_dim), device=self.device)\n",
    "\n",
    "        # news_embedding_seq,_ = self.lstm(news_embedding_set)\n",
    "        # news_embedding_dilations[:,:,0,:] = news_embedding_seq\n",
    "\n",
    "        news_embedding_set = news_embedding_set.transpose(-2,-1)\n",
    "\n",
    "        news_embedding_d1 = self.CNN_d1(news_embedding_set)\n",
    "        news_embedding_d1 = self.LayerNorm(news_embedding_d1.transpose(-2,-1))\n",
    "        news_embedding_dilations[:,:,0,:] = self.ReLU(news_embedding_d1)\n",
    "\n",
    "        news_embedding_d2 = self.CNN_d2(news_embedding_set)\n",
    "        news_embedding_d2 = self.LayerNorm(news_embedding_d2.transpose(-2,-1))\n",
    "        news_embedding_dilations[:,:,1,:] = self.ReLU(news_embedding_d2)\n",
    "\n",
    "        news_embedding_d3 = self.CNN_d3(news_embedding_set)\n",
    "        news_embedding_d3 = self.LayerNorm(news_embedding_d3.transpose(-2,-1))\n",
    "        news_embedding_dilations[:,:,2,:] = self.ReLU(news_embedding_d3)\n",
    "\n",
    "        return news_embedding_dilations\n",
    "\n",
    "    def forward(self, news_batch, **kwargs):\n",
    "        \"\"\" encode set of news to news representation\n",
    "\n",
    "        Args:\n",
    "            news_batch: batch of news tokens, of size [batch_size, *, signal_length]\n",
    "\n",
    "        Returns:\n",
    "            news_embedding: hidden vector of each token in news, of size [batch_size, *, signal_length, level, hidden_dim]\n",
    "            news_repr: hidden vector of each news, of size [batch_size, *, hidden_dim]\n",
    "        \"\"\"\n",
    "        news_embedding_pretrained = self.DropOut(\n",
    "            self.embedding(news_batch)).view(-1, news_batch.shape[2], self.embedding_dim)\n",
    "        news_embedding = self._HDC(news_embedding_pretrained).view(\n",
    "            news_batch.shape + (self.level-1, self.hidden_dim))\n",
    "        news_embedding_attn = Attention.ScaledDpAttention(\n",
    "            self.query_levels, news_embedding, news_embedding).squeeze(dim=-2)\n",
    "        news_repr = Attention.ScaledDpAttention(self.query_words, news_embedding_attn, news_embedding_attn).squeeze(\n",
    "            dim=-2).view(news_batch.shape[0], news_batch.shape[1], self.hidden_dim)\n",
    "        news_repr_selection = self.selectionProject(news_repr)\n",
    "        news_embedding_selection,_ = self.lstm(news_embedding_pretrained.view(-1, news_batch.shape[-1], self.embedding_dim),(news_repr_selection.view(1,-1,self.hidden_dim), torch.zeros((1,news_batch.size(0)*news_batch.size(1),self.hidden_dim),device=self.device)))\n",
    "        news_embedding = torch.cat([news_embedding_selection.view(news_batch.shape + (1,self.hidden_dim)),news_embedding],dim=-2)\n",
    "\n",
    "        return news_embedding, news_repr, news_repr_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = FIM_Encoder(hparams, vocab)\n",
    "# interactor = KNRM_Interactor()\n",
    "\n",
    "hparams['name'] = '-'.join([name,encoder.name,hparams['select']])\n",
    "\n",
    "# sfi = SFI_gating_MultiView(hparams, encoder, interactor).to(hparams['device'])\n",
    "sfi = SFI_gating(hparams, encoder).to(hparams['device'])\n",
    "\n",
    "# sfi.load_state_dict(torch.load('/home/peitian_zhang/Codes/News-Recommendation/data/model_params/sfi-fim-fim-gating/large_epoch4_step33832_[hs=50,topk=30,attrs=title].model', map_location=hparams['device'])['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sfi.encoder.selectionProject.weight.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0928,  0.0415, -0.1030,  ..., -0.1363, -0.0342,  0.0186],\n",
       "        [-0.0662,  0.0276,  0.1351,  ...,  0.0953,  0.0177, -0.1659],\n",
       "        [ 0.0164, -0.0031,  0.0452,  ...,  0.0770,  0.0047,  0.0582],\n",
       "        ...,\n",
       "        [-0.0755, -0.1510,  0.0390,  ..., -0.1298,  0.0647, -0.0166],\n",
       "        [-0.1098,  0.0861,  0.0469,  ..., -0.0462, -0.0411,  0.0489],\n",
       "        [ 0.1958, -0.1618, -0.1387,  ...,  0.0561, -0.0147, -0.0092]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.2239, -1.4873, -1.5518, -1.9387, -2.0860],\n",
       "        [-1.8369, -2.2684, -1.4035, -1.4035, -1.4035],\n",
       "        [-2.0791, -1.3006, -1.6752, -1.1655, -2.2678],\n",
       "        [-2.4069, -1.2226, -1.8529, -1.1013, -2.0698],\n",
       "        [-2.4358, -1.1874, -1.9216, -2.0400, -1.1055],\n",
       "        [-1.3759, -2.6017, -1.3875, -1.7756, -1.3697],\n",
       "        [-1.9593, -1.8125, -1.4701, -2.0441, -1.0895],\n",
       "        [-1.1525, -1.3465, -1.8123, -1.7633, -2.4164],\n",
       "        [-1.2692, -1.9558, -1.0408, -2.2987, -2.0881],\n",
       "        [-1.9164, -0.8806, -1.8325, -3.0058, -1.4749]], device='cuda:0',\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "sfi(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-04-21 16:26:37,578] INFO (root) training...\n",
      "epoch 1 , step 290 , loss: 1.5594: 100%|██████████| 295/295 [00:20<00:00, 14.45it/s]\n",
      "[2021-04-21 16:26:58,968] INFO (root) saved model of step 0, epoch 1 at data/model_params/sfi-fim-gating/demo_epoch1_step0_[hs=50,topk=30,attrs=title].model\n",
      "[2021-04-21 16:26:59,055] INFO (root) evaluating...\n",
      "100%|██████████| 1812/1812 [00:46<00:00, 39.21it/s]\n",
      "[2021-04-21 16:27:45,786] INFO (root) evaluation results:{'auc': 0.5432, 'mean_mrr': 0.2457, 'ndcg@5': 0.2554, 'ndcg@10': 0.3265, 'epoch': 1, 'step': 0}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SFI_gating(\n",
       "  (encoder): FIM_Encoder(\n",
       "    (embedding): Embedding(54316, 300, sparse=True)\n",
       "    (ReLU): ReLU()\n",
       "    (LayerNorm): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "    (DropOut): Dropout(p=0.2, inplace=False)\n",
       "    (CNN_d1): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (CNN_d2): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (CNN_d3): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "    (lstm): LSTM(300, 150, batch_first=True)\n",
       "    (selectionProject): Linear(in_features=150, out_features=150, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (interactor): FIM_Interactor(\n",
       "    (SeqCNN3D): Sequential(\n",
       "      (0): Conv3d(4, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool3d(kernel_size=[3, 3, 3], stride=[3, 3, 3], padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv3d(32, 16, kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (4): ReLU()\n",
       "      (5): MaxPool3d(kernel_size=[3, 3, 3], stride=[3, 3, 3], padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (learningToRank): Linear(in_features=192, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "hparams['epochs'] = 1\n",
    "train(sfi, hparams, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= a==sfi.encoder.selectionProject.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0823,  0.0512, -0.0861,  ..., -0.1184, -0.0415,  0.0027],\n",
       "        [-0.0967,  0.0019,  0.0927,  ...,  0.0536, -0.0065, -0.1693],\n",
       "        [-0.0074, -0.0223,  0.0086,  ...,  0.0305,  0.0033,  0.0437],\n",
       "        ...,\n",
       "        [-0.0722, -0.1471,  0.0411,  ..., -0.1110,  0.0561, -0.0364],\n",
       "        [-0.0979,  0.0891,  0.0523,  ..., -0.0213, -0.0411,  0.0343],\n",
       "        [ 0.2000, -0.1572, -0.1392,  ...,  0.0425, -0.0041,  0.0112]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "sfi.encoder.selectionProject.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.0928,  0.0415, -0.1030,  ...,  0.0561, -0.0147, -0.0092],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "a[c==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}